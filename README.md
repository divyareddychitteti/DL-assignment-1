# ğŸ§  MNIST Feedforward Neural Network

This is a flexible implementation of a **feedforward neural network** using **PyTorch**, built for CBIT's Deep Learning Assignment 1. The network is trained on the **MNIST** dataset and supports multiple optimizers, activation functions, and hyperparameters.

---

## ğŸ“ Project Structure

. â”œâ”€â”€ main.py # Entry point to train and evaluate model â”œâ”€â”€ model.py # Customizable neural network architecture â”œâ”€â”€ train.py # Training + validation logic â”œâ”€â”€ requirements.txt # Required packages â””â”€â”€ README.md # Project instructions

yaml
Copy
Edit

---

## âš™ï¸ Features

- âœ… Custom hidden layers (3, 4, 5 layers)
- âœ… Flexible neuron count (32, 64, 128 per layer)
- âœ… Activation functions: ReLU / Sigmoid
- âœ… Optimizers: `sgd`, `momentum`, `nesterov`, `rmsprop`, `adam`, `nadam`
- âœ… Hyperparameter tuning support (batch size, learning rate, weight decay)
- âœ… Train/Validation/Test split (90/10/10)
- âœ… Accuracy evaluation on validation + test set

---

## ğŸ”§ Setup & Run

1. **Install packages:**
   ```bash
   pip install -r requirements.txt
Train the model:

bash
Copy
Edit
python main.py
ğŸ“Š Evaluation
Uses CrossEntropyLoss

Accuracy printed per epoch

Supports comparison with MSELoss (optional)

Confusion matrix plotting (optional)

ğŸ“Œ Assignment Info
ğŸ“š Subject: Deep Learning (22CAC04)

ğŸ§‘â€ğŸ“ Roll No: 143

ğŸ“ Dataset: MNIST

ğŸ§ª Goal: Build and train a feedforward neural network using backpropagation
